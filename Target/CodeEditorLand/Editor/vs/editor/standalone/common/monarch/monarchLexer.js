var K=Object.defineProperty;var G=Object.getOwnPropertyDescriptor;var O=(x,e,t,n)=>{for(var a=n>1?void 0:n?G(e,t):e,r=x.length-1,u;r>=0;r--)(u=x[r])&&(a=(n?u(e,t,a):u(a))||a);return n&&a&&K(e,t,a),a},B=(x,e)=>(t,n)=>e(t,n,x);import{Disposable as H}from"../../../../base/common/lifecycle.js";import*as L from"../../../common/languages.js";import{NullState as q,nullTokenizeEncoded as V,nullTokenize as J}from"../../../common/languages/nullTokenize.js";import"../../../common/languages/supports/tokenization.js";import"../../../common/languages/language.js";import*as s from"./monarchCommon.js";import"../standaloneTheme.js";import{IConfigurationService as Q}from"../../../../platform/configuration/common/configuration.js";import{LanguageId as X,MetadataConsts as Y}from"../../../common/encodedTokenAttributes.js";const F=5;class C{static _INSTANCE=new C(F);static create(e,t){return this._INSTANCE.create(e,t)}_maxCacheDepth;_entries;constructor(e){this._maxCacheDepth=e,this._entries=Object.create(null)}create(e,t){if(e!==null&&e.depth>=this._maxCacheDepth)return new v(e,t);let n=v.getStackElementId(e);n.length>0&&(n+="|"),n+=t;let a=this._entries[n];return a||(a=new v(e,t),this._entries[n]=a,a)}}class v{parent;state;depth;constructor(e,t){this.parent=e,this.state=t,this.depth=(this.parent?this.parent.depth:0)+1}static getStackElementId(e){let t="";for(;e!==null;)t.length>0&&(t+="|"),t+=e.state,e=e.parent;return t}static _equals(e,t){for(;e!==null&&t!==null;){if(e===t)return!0;if(e.state!==t.state)return!1;e=e.parent,t=t.parent}return e===null&&t===null}equals(e){return v._equals(this,e)}push(e){return C.create(this,e)}pop(){return this.parent}popall(){let e=this;for(;e.parent;)e=e.parent;return e}switchTo(e){return C.create(this.parent,e)}}class y{languageId;state;constructor(e,t){this.languageId=e,this.state=t}equals(e){return this.languageId===e.languageId&&this.state.equals(e.state)}clone(){return this.state.clone()===this.state?this:new y(this.languageId,this.state)}}class E{static _INSTANCE=new E(F);static create(e,t){return this._INSTANCE.create(e,t)}_maxCacheDepth;_entries;constructor(e){this._maxCacheDepth=e,this._entries=Object.create(null)}create(e,t){if(t!==null)return new w(e,t);if(e!==null&&e.depth>=this._maxCacheDepth)return new w(e,t);const n=v.getStackElementId(e);let a=this._entries[n];return a||(a=new w(e,null),this._entries[n]=a,a)}}class w{stack;embeddedLanguageData;constructor(e,t){this.stack=e,this.embeddedLanguageData=t}clone(){return(this.embeddedLanguageData?this.embeddedLanguageData.clone():null)===this.embeddedLanguageData?this:E.create(this.stack,this.embeddedLanguageData)}equals(e){return!(e instanceof w)||!this.stack.equals(e.stack)?!1:this.embeddedLanguageData===null&&e.embeddedLanguageData===null?!0:this.embeddedLanguageData===null||e.embeddedLanguageData===null?!1:this.embeddedLanguageData.equals(e.embeddedLanguageData)}}class Z{_tokens;_languageId;_lastTokenType;_lastTokenLanguage;constructor(){this._tokens=[],this._languageId=null,this._lastTokenType=null,this._lastTokenLanguage=null}enterLanguage(e){this._languageId=e}emit(e,t){this._lastTokenType===t&&this._lastTokenLanguage===this._languageId||(this._lastTokenType=t,this._lastTokenLanguage=this._languageId,this._tokens.push(new L.Token(e,t,this._languageId)))}nestedLanguageTokenize(e,t,n,a){const r=n.languageId,u=n.state,g=L.TokenizationRegistry.get(r);if(!g)return this.enterLanguage(r),this.emit(a,""),u;const l=g.tokenize(e,t,u);if(a!==0)for(const c of l.tokens)this._tokens.push(new L.Token(c.offset+a,c.type,c.language));else this._tokens=this._tokens.concat(l.tokens);return this._lastTokenType=null,this._lastTokenLanguage=null,this._languageId=null,l.endState}finalize(e){return new L.TokenizationResult(this._tokens,e)}}class R{_languageService;_theme;_prependTokens;_tokens;_currentLanguageId;_lastTokenMetadata;constructor(e,t){this._languageService=e,this._theme=t,this._prependTokens=null,this._tokens=[],this._currentLanguageId=X.Null,this._lastTokenMetadata=0}enterLanguage(e){this._currentLanguageId=this._languageService.languageIdCodec.encodeLanguageId(e)}emit(e,t){const n=this._theme.match(this._currentLanguageId,t)|Y.BALANCED_BRACKETS_MASK;this._lastTokenMetadata!==n&&(this._lastTokenMetadata=n,this._tokens.push(e),this._tokens.push(n))}static _merge(e,t,n){const a=e!==null?e.length:0,r=t.length,u=n!==null?n.length:0;if(a===0&&r===0&&u===0)return new Uint32Array(0);if(a===0&&r===0)return n;if(r===0&&u===0)return e;const g=new Uint32Array(a+r+u);e!==null&&g.set(e);for(let l=0;l<r;l++)g[a+l]=t[l];return n!==null&&g.set(n,a+r),g}nestedLanguageTokenize(e,t,n,a){const r=n.languageId,u=n.state,g=L.TokenizationRegistry.get(r);if(!g)return this.enterLanguage(r),this.emit(a,""),u;const l=g.tokenizeEncoded(e,t,u);if(a!==0)for(let c=0,f=l.tokens.length;c<f;c+=2)l.tokens[c]+=a;return this._prependTokens=R._merge(this._prependTokens,this._tokens,l.tokens),this._tokens=[],this._currentLanguageId=0,this._lastTokenMetadata=0,l.endState}finalize(e){return new L.EncodedTokenizationResult(R._merge(this._prependTokens,this._tokens,null),e)}}let z=class extends H{constructor(t,n,a,r,u){super();this._configurationService=u;this._languageService=t,this._standaloneThemeService=n,this._languageId=a,this._lexer=r,this._embeddedLanguages=Object.create(null),this.embeddedLoaded=Promise.resolve(void 0);let g=!1;this._register(L.TokenizationRegistry.onDidChange(l=>{if(g)return;let c=!1;for(let f=0,d=l.changedLanguages.length;f<d;f++){const p=l.changedLanguages[f];if(this._embeddedLanguages[p]){c=!0;break}}c&&(g=!0,L.TokenizationRegistry.handleChange([this._languageId]),g=!1)})),this._maxTokenizationLineLength=this._configurationService.getValue("editor.maxTokenizationLineLength",{overrideIdentifier:this._languageId}),this._register(this._configurationService.onDidChangeConfiguration(l=>{l.affectsConfiguration("editor.maxTokenizationLineLength")&&(this._maxTokenizationLineLength=this._configurationService.getValue("editor.maxTokenizationLineLength",{overrideIdentifier:this._languageId}))}))}_languageService;_standaloneThemeService;_languageId;_lexer;_embeddedLanguages;embeddedLoaded;_maxTokenizationLineLength;getLoadStatus(){const t=[];for(const n in this._embeddedLanguages){const a=L.TokenizationRegistry.get(n);if(a){if(a instanceof z){const r=a.getLoadStatus();r.loaded===!1&&t.push(r.promise)}continue}L.TokenizationRegistry.isResolved(n)||t.push(L.TokenizationRegistry.getOrCreate(n))}return t.length===0?{loaded:!0}:{loaded:!1,promise:Promise.all(t).then(n=>{})}}getInitialState(){const t=C.create(null,this._lexer.start);return E.create(t,null)}tokenize(t,n,a){if(t.length>=this._maxTokenizationLineLength)return J(this._languageId,a);const r=new Z,u=this._tokenize(t,n,a,r);return r.finalize(u)}tokenizeEncoded(t,n,a){if(t.length>=this._maxTokenizationLineLength)return V(this._languageService.languageIdCodec.encodeLanguageId(this._languageId),a);const r=new R(this._languageService,this._standaloneThemeService.getColorTheme().tokenTheme),u=this._tokenize(t,n,a,r);return r.finalize(u)}_tokenize(t,n,a,r){return a.embeddedLanguageData?this._nestedTokenize(t,n,a,0,r):this._myTokenize(t,n,a,0,r)}_findLeavingNestedLanguageOffset(t,n){let a=this._lexer.tokenizer[n.stack.state];if(!a&&(a=s.findRules(this._lexer,n.stack.state),!a))throw s.createError(this._lexer,"tokenizer state is not defined: "+n.stack.state);let r=-1,u=!1;for(const g of a){if(!s.isIAction(g.action)||g.action.nextEmbedded!=="@pop")continue;u=!0;let l=g.resolveRegex(n.stack.state);const c=l.source;if(c.substr(0,4)==="^(?:"&&c.substr(c.length-1,1)===")"){const d=(l.ignoreCase?"i":"")+(l.unicode?"u":"");l=new RegExp(c.substr(4,c.length-5),d)}const f=t.search(l);f===-1||f!==0&&g.matchOnlyAtLineStart||(r===-1||f<r)&&(r=f)}if(!u)throw s.createError(this._lexer,'no rule containing nextEmbedded: "@pop" in tokenizer embedded state: '+n.stack.state);return r}_nestedTokenize(t,n,a,r,u){const g=this._findLeavingNestedLanguageOffset(t,a);if(g===-1){const f=u.nestedLanguageTokenize(t,n,a.embeddedLanguageData,r);return E.create(a.stack,new y(a.embeddedLanguageData.languageId,f))}const l=t.substring(0,g);l.length>0&&u.nestedLanguageTokenize(l,!1,a.embeddedLanguageData,r);const c=t.substring(g);return this._myTokenize(c,n,a,r+g,u)}_safeRuleName(t){return t?t.name:"(unknown)"}_myTokenize(t,n,a,r,u){u.enterLanguage(this._languageId);const g=t.length,l=n&&this._lexer.includeLF?t+`
`:t,c=l.length;let f=a.embeddedLanguageData,d=a.stack,p=0,b=null,D=!0;for(;D||p<c;){const A=p,P=d.depth,U=b?b.groups.length:0,S=d.state;let h=null,m=null,i=null,T=null,M=null;if(b){h=b.matches;const o=b.groups.shift();m=o.matched,i=o.action,T=b.rule,b.groups.length===0&&(b=null)}else{if(!D&&p>=c)break;D=!1;let o=this._lexer.tokenizer[S];if(!o&&(o=s.findRules(this._lexer,S),!o))throw s.createError(this._lexer,"tokenizer state is not defined: "+S);const k=l.substr(p);for(const I of o)if((p===0||!I.matchOnlyAtLineStart)&&(h=k.match(I.resolveRegex(S)),h)){m=h[0],i=I.action;break}}if(h||(h=[""],m=""),i||(p<c&&(h=[l.charAt(p)],m=h[0]),i=this._lexer.defaultToken),m===null)break;for(p+=m.length;s.isFuzzyAction(i)&&s.isIAction(i)&&i.test;)i=i.test(m,h,S,p===c);let _=null;if(typeof i=="string"||Array.isArray(i))_=i;else if(i.group)_=i.group;else if(i.token!==null&&i.token!==void 0){if(i.tokenSubst?_=s.substituteMatches(this._lexer,i.token,m,h,S):_=i.token,i.nextEmbedded)if(i.nextEmbedded==="@pop"){if(!f)throw s.createError(this._lexer,"cannot pop embedded language if not inside one");f=null}else{if(f)throw s.createError(this._lexer,"cannot enter embedded language from within an embedded language");M=s.substituteMatches(this._lexer,i.nextEmbedded,m,h,S)}if(i.goBack&&(p=Math.max(0,p-i.goBack)),i.switchTo&&typeof i.switchTo=="string"){let o=s.substituteMatches(this._lexer,i.switchTo,m,h,S);if(o[0]==="@"&&(o=o.substr(1)),s.findRules(this._lexer,o))d=d.switchTo(o);else throw s.createError(this._lexer,"trying to switch to a state '"+o+"' that is undefined in rule: "+this._safeRuleName(T))}else{if(i.transform&&typeof i.transform=="function")throw s.createError(this._lexer,"action.transform not supported");if(i.next)if(i.next==="@push"){if(d.depth>=this._lexer.maxStack)throw s.createError(this._lexer,"maximum tokenizer stack size reached: ["+d.state+","+d.parent.state+",...]");d=d.push(S)}else if(i.next==="@pop"){if(d.depth<=1)throw s.createError(this._lexer,"trying to pop an empty stack in rule: "+this._safeRuleName(T));d=d.pop()}else if(i.next==="@popall")d=d.popall();else{let o=s.substituteMatches(this._lexer,i.next,m,h,S);if(o[0]==="@"&&(o=o.substr(1)),s.findRules(this._lexer,o))d=d.push(o);else throw s.createError(this._lexer,"trying to set a next state '"+o+"' that is undefined in rule: "+this._safeRuleName(T))}}i.log&&typeof i.log=="string"&&s.log(this._lexer,this._lexer.languageId+": "+s.substituteMatches(this._lexer,i.log,m,h,S))}if(_===null)throw s.createError(this._lexer,"lexer rule has no well-defined action in rule: "+this._safeRuleName(T));const N=o=>{const k=this._languageService.getLanguageIdByLanguageName(o)||this._languageService.getLanguageIdByMimeType(o)||o,I=this._getNestedEmbeddedLanguageData(k);if(p<c){const j=t.substr(p);return this._nestedTokenize(j,n,E.create(d,I),r+p,u)}else return E.create(d,I)};if(Array.isArray(_)){if(b&&b.groups.length>0)throw s.createError(this._lexer,"groups cannot be nested: "+this._safeRuleName(T));if(h.length!==_.length+1)throw s.createError(this._lexer,"matched number of groups does not match the number of actions in rule: "+this._safeRuleName(T));let o=0;for(let k=1;k<h.length;k++)o+=h[k].length;if(o!==m.length)throw s.createError(this._lexer,"with groups, all characters should be matched in consecutive groups in rule: "+this._safeRuleName(T));b={rule:T,matches:h,groups:[]};for(let k=0;k<_.length;k++)b.groups[k]={action:_[k],matched:h[k+1]};p-=m.length;continue}else{if(_==="@rematch"&&(p-=m.length,m="",h=null,_="",M!==null))return N(M);if(m.length===0){if(c===0||P!==d.depth||S!==d.state||(b?b.groups.length:0)!==U)continue;throw s.createError(this._lexer,"no progress in tokenizer in rule: "+this._safeRuleName(T))}let o=null;if(s.isString(_)&&_.indexOf("@brackets")===0){const k=_.substr(9),I=$(this._lexer,m);if(!I)throw s.createError(this._lexer,"@brackets token returned but no bracket defined as: "+m);o=s.sanitize(I.token+k)}else{const k=_===""?"":_+this._lexer.tokenPostfix;o=s.sanitize(k)}A<g&&u.emit(A+r,o)}if(M!==null)return N(M)}return E.create(d,f)}_getNestedEmbeddedLanguageData(t){if(!this._languageService.isRegisteredLanguageId(t))return new y(t,q);t!==this._languageId&&(this._languageService.requestBasicLanguageFeatures(t),L.TokenizationRegistry.getOrCreate(t),this._embeddedLanguages[t]=!0);const n=L.TokenizationRegistry.get(t);return n?new y(t,n.getInitialState()):new y(t,q)}};z=O([B(4,Q)],z);function $(x,e){if(!e)return null;e=s.fixCase(x,e);const t=x.brackets;for(const n of t){if(n.open===e)return{token:n.token,bracketType:s.MonarchBracket.Open};if(n.close===e)return{token:n.token,bracketType:s.MonarchBracket.Close}}return null}export{z as MonarchTokenizer};
