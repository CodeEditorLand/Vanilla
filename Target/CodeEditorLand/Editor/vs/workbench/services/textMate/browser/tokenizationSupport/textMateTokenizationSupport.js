import{Emitter as m}from"../../../../../base/common/event.js";import{Disposable as T}from"../../../../../base/common/lifecycle.js";import{StopWatch as S}from"../../../../../base/common/stopwatch.js";import{TokenMetadata as z}from"../../../../../editor/common/encodedTokenAttributes.js";import{EncodedTokenizationResult as k}from"../../../../../editor/common/languages.js";import"../../../../../editor/common/model.js";class V extends T{constructor(e,a,t,r,i,l,n){super();this._grammar=e;this._initialState=a;this._containsEmbeddedLanguages=t;this._createBackgroundTokenizer=r;this._backgroundTokenizerShouldOnlyVerifyTokens=i;this._reportTokenizationTime=l;this._reportSlowTokenization=n}_seenLanguages=[];_onDidEncounterLanguage=this._register(new m);onDidEncounterLanguage=this._onDidEncounterLanguage.event;get backgroundTokenizerShouldOnlyVerifyTokens(){return this._backgroundTokenizerShouldOnlyVerifyTokens()}getInitialState(){return this._initialState}tokenize(e,a,t){throw new Error("Not supported!")}createBackgroundTokenizer(e,a){if(this._createBackgroundTokenizer)return this._createBackgroundTokenizer(e,a)}tokenizeEncoded(e,a,t){const r=Math.random()*1e4<1,i=this._reportSlowTokenization||r,l=i?new S(!0):void 0,n=this._grammar.tokenizeLine2(e,t,500);if(i){const o=l.elapsed();(r||o>32)&&this._reportTokenizationTime(o,e.length,r)}if(n.stoppedEarly)return console.warn(`Time limit reached when tokenizing line: ${e.substring(0,100)}`),new k(n.tokens,t);if(this._containsEmbeddedLanguages){const o=this._seenLanguages,g=n.tokens;for(let u=0,c=g.length>>>1;u<c;u++){const p=g[(u<<1)+1],s=z.getLanguageId(p);o[s]||(o[s]=!0,this._onDidEncounterLanguage.fire(s))}}let d;return t.equals(n.ruleStack)?d=t:d=n.ruleStack,new k(n.tokens,d)}}export{V as TextMateTokenizationSupport};
