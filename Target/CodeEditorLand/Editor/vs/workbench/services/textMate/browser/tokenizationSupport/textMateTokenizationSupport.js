import{Emitter as m}from"../../../../../base/common/event.js";import{Disposable as T}from"../../../../../base/common/lifecycle.js";import{StopWatch as S}from"../../../../../base/common/stopwatch.js";import{TokenMetadata as z}from"../../../../../editor/common/encodedTokenAttributes.js";import{EncodedTokenizationResult as k}from"../../../../../editor/common/languages.js";class E extends T{constructor(n,a,t,i,r,l,e){super();this._grammar=n;this._initialState=a;this._containsEmbeddedLanguages=t;this._createBackgroundTokenizer=i;this._backgroundTokenizerShouldOnlyVerifyTokens=r;this._reportTokenizationTime=l;this._reportSlowTokenization=e}_seenLanguages=[];_onDidEncounterLanguage=this._register(new m);onDidEncounterLanguage=this._onDidEncounterLanguage.event;get backgroundTokenizerShouldOnlyVerifyTokens(){return this._backgroundTokenizerShouldOnlyVerifyTokens()}getInitialState(){return this._initialState}tokenize(n,a,t){throw new Error("Not supported!")}createBackgroundTokenizer(n,a){if(this._createBackgroundTokenizer)return this._createBackgroundTokenizer(n,a)}tokenizeEncoded(n,a,t){const i=Math.random()*1e4<1,r=this._reportSlowTokenization||i,l=r?new S(!0):void 0,e=this._grammar.tokenizeLine2(n,t,500);if(r){const o=l.elapsed();(i||o>32)&&this._reportTokenizationTime(o,n.length,i)}if(e.stoppedEarly)return new k(e.tokens,t);if(this._containsEmbeddedLanguages){const o=this._seenLanguages,g=e.tokens;for(let u=0,c=g.length>>>1;u<c;u++){const p=g[(u<<1)+1],s=z.getLanguageId(p);o[s]||(o[s]=!0,this._onDidEncounterLanguage.fire(s))}}let d;return t.equals(e.ruleStack)?d=t:d=e.ruleStack,new k(e.tokens,d)}}export{E as TextMateTokenizationSupport};
